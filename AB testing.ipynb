{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76393f94-b656-464d-87c7-37681d77f436",
   "metadata": {},
   "source": [
    "# A/B testing\n",
    "\n",
    "An A/B Test is a scientific method of choosing between two options (Option A and Option B). Some examples of A/B tests include:\n",
    "\n",
    "* What number of sale items on a website makes customers most likely to purchase something: 25 or 50?\n",
    "* What color button are customers more likely to click on: blue or green?\n",
    "* Do people spend more time on a website if the background is green or orange?\n",
    "\n",
    "For A/B tests where the outcome of interest (eg., whether or not a customer makes a purchase) is categorical, an A/B test is conducted using a Chi-Square hypothesis test. In order to determine the sample size necessary for this kind of test, a sample size calculator requires three numbers:\n",
    "\n",
    "* Baseline conversion rate\n",
    "* Minimum detectable effect (also called the minimum desired lift)\n",
    "* Statistical significance threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f07594-e090-47c8-8b13-be75f4d25634",
   "metadata": {},
   "source": [
    "## Baseline Conversion Rate\n",
    "\n",
    "A/B tests usually compare an option that we’re currently using to a new option that we suspect might be better. In order to compare the two options, we need a metric. Often, our metric will be the percent of users who take a certain action after interacting with one of our options. For instance:\n",
    "\n",
    "* The percent of customers who buy a t-shirt after visiting one of two versions of a website\n",
    "* The percent of users who click on one of two versions of an ad\n",
    "* In the t-shirt example above, the baseline conversion rate is our estimate for the percent of people who will buy a shirt under the current website design.\n",
    "\n",
    "We can generally calculate a baseline by looking at historical data for the option that we’re currently using. For example, suppose that 2000 people visited a website over the past three months and 320 of those visitors purchased a shirt. We could estimate the baseline rate as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb10f309-73a3-475e-9074-c1323c2b7a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n"
     ]
    }
   ],
   "source": [
    "baseline = 320/2000*100\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58757684-d167-4cea-be65-6370206f2863",
   "metadata": {},
   "source": [
    "This number may be written as a proportion (eg., 0.16) or a percent (eg., 16%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228fdc16-a416-41f5-a492-6f5ebd1ef72a",
   "metadata": {},
   "source": [
    "## Minimum Detectable Effect\n",
    "\n",
    "Suppose we’re running an A/B Test to find out if a new website layout drives more subscriptions than the current one. If the new layout is only a tiny percent better, would we really care?\n",
    "\n",
    "In order to detect precise differences, we need a very large sample size. In order to choose a sample size, we need to know the smallest difference that we actually care to measure. This “smallest difference” is our desired minimum detectable effect. This is also sometimes referred to as desired lift.\n",
    "\n",
    "Minimum detectable effect or lift is generally expressed as a percent of the baseline conversion rate. Suppose that 6% of customers currently subscribe to our website (that’s our baseline conversion rate). Changing a website layout is hard, so we only think that it’s worth doing if at least 8% of our customers would subscribe with the new layout. To calculate this as a percentage of our baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e4187d-0b25-49f3-8c11-042c9065f0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.33333333333333\n"
     ]
    }
   ],
   "source": [
    "baseline = 6\n",
    "new = 8\n",
    "min_detectable_effect = (new - baseline) / baseline * 100\n",
    "print(min_detectable_effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b21a89-0412-4063-a4bd-0cd3936b5465",
   "metadata": {},
   "source": [
    "## Significance Threshold\n",
    "\n",
    "When we run an A/B test, we usually want to use the results of the test to make a decision: use version A or B? In order to make that decision, many data scientists use a pre-determined significance threshold for their hypothesis test. For example, if we set a significance threshold of 0.05 (a commonly chosen value), we’ll “reject the null hypothesis” and conclude that the conversion rate for version B is significantly different from version A if we get a p-value less than 0.05.\n",
    "\n",
    "It turns out that this significance threshold is the false positive rate for the test: the probability of finding a significant difference when there really is none. As a business owner, we don’t want to make this kind of mistake, because then we might invest money in a change that doesn’t actually make a difference!\n",
    "\n",
    "Unfortunately, there’s a trade-off between false positives and false negatives. A false negative occurs when there is a difference between version A and B, but the test doesn’t detect it. This is a potential missed opportunity for a business owner!\n",
    "\n",
    "Most A/B test sample size calculators estimate the sample size needed for a 20% false negative rate; while a data scientist needs to choose the false positive rate they are comfortable with. The lower the false positive rate, the larger the sample size will need to be!\n",
    "\n",
    "## Don't Interfere With Your Tests\n",
    "Suppose that a Product Manager is running an A/B Test for a redesign of a landing page. Before starting the test, she used a sample size calculator to determine the sample size: 2,200 total website visitors. After reaching 2,200 visits, she ran a Chi-Square Test. The new website design performed slightly better, but the results were not statistically significant.\n",
    "\n",
    "It might be tempting to run the test for another week to see if the difference becomes significant, but that would be a big mistake! By choosing to extend the A/B test past the original sample size, the project manager would introduce personal bias to the results of the test; she will be more likely to get the results she wants, regardless if these results reflect reality.\n",
    "\n",
    "Here are two important rules for making sure that A/B tests remain unbiased:\n",
    "\n",
    "Don’t continue to run the test after the predetermined sample size, until “significant” results are found.\n",
    "Don’t stop a test before reaching the predetermined sample size, just because your results reach significance early (unless there are ethical reasons that require you to stop, like a prescription drug trial).\n",
    "Test data is sensitive to changes in sample size, which is why it is important to calculate beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277d8caa-eac6-4c11-bdde-b0de9ff53801",
   "metadata": {},
   "source": [
    "## Sample size determination with simulation\n",
    "\n",
    "Suppose that a media company currently has a weekly newsletter email and wants to see if using the recipient’s first name in the email subject will cause more people to open the email (ie. “Bob! Checkout this week’s updates” vs “Checkout this week’s updates”). They randomly assign a group of 100 recipients to receive one of the two email subjects and record whether or not each recipient opened the email. \n",
    "\n",
    "In order to run a hypothesis test to decide whether there is a significant difference in the open rate for these emails, we would run a Chi-Square test. To accomplish this, we would first create a contingency table for the Email and Opened variables in the above table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5e4c6-5473-4312-9658-26ffda91d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contingency table\n",
    "X = pd.crosstab(data.Email, data.Opened)\n",
    "print(X)\n",
    "\n",
    "#Chi square test\n",
    "chi2, pval, dof, expected = chi2_contingency(X)\n",
    "print(pval) #Output: 0.2186"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551741c9-aba6-4617-9355-436ae74ac79f",
   "metadata": {},
   "source": [
    "Based on the p-value, we would make a decision about which email to use; a small p-value would provide evidence that the open rates are significantly different for the two groups, while a large p-value would suggest no significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e8f5bc-6957-49e6-863e-1b8059eeeb50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
