{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb950a3-9bf0-4c10-8b4c-7c6a99aaa2d2",
   "metadata": {},
   "source": [
    "# Encoding categorical data\n",
    "\n",
    "Categorical data is data that has more than one category. When working with that type of data we have two types, nominal and ordinal. Nominal data is data that has no particular order or hierarchy to it, and ordinal data is categorical data where the categories have order, but the differences between the categories are not important or unclear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294178fb-1786-4b9e-a659-5acdf9b597f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import data\n",
    "cars = pd.read_csv('cars.csv')\n",
    "\n",
    "# check variable types\n",
    "print(cars.dtypes)\n",
    "## OUTPUT\n",
    "# year              int64\n",
    "# make             object\n",
    "# model            object\n",
    "# trim             object\n",
    "# body             object\n",
    "# transmission     object\n",
    "# vin              object\n",
    "# state            object\n",
    "# condition        object\n",
    "# odometer        float64\n",
    "# color            object\n",
    "# interior         object\n",
    "# seller           object\n",
    "# mmr               int64\n",
    "# sellingprice      int64\n",
    "# saledate         object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6399720-7f4d-46c5-a5f1-e18dbfaaa799",
   "metadata": {},
   "source": [
    "We can see from the output that we have a lot of features that are dtype = object and that tells us those features could be text or a mix of text and numerical values. For our encoding examples, we will explore a few of those object features and transform those values so we can have a data frame ready for machine learning.\n",
    "\n",
    "The reason we put our time into this level of encoding is that there are many machine learning models that cannot handle text and will only work with numbers. Our data must be encoded into numbers before we even begin to train, test, or evaluate a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad00511-fa97-4e51-8326-552a6013ece7",
   "metadata": {},
   "source": [
    "## Ordinal encoding\n",
    "\n",
    "We mentioned already that ordinal data is data that does have order and a hierarchy between its values. Let us take a look at the condition feature from our data frame and perform a value_counts to see how many times each label is listed in our feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27fd5f4-a2ad-42ae-956a-11c4f968ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cars['condition'].value_counts())\n",
    "# #OUTPUT\n",
    "# New          2881\n",
    "# Like New     2860\n",
    "# Good         2027\n",
    "# Fair          753\n",
    "# Excellent     186"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fed15a-e88d-4e1e-8572-3681b8d3c27c",
   "metadata": {},
   "source": [
    "his is definitely an example of ordinal data: the condition of the used cars can easily be put in order of those in the “best” condition to the cars in the “worst” condition. The output printed the labels with the highest counts, but we can assume the following hierarchy:\n",
    "\n",
    "* Excellent\n",
    "* New\n",
    "* Like New\n",
    "* Good\n",
    "* Fair\n",
    "\n",
    "We need to convert these labels into numbers, and we can do this with two different approaches. First, we can do this by creating a dictionary where every label is the key and the new numeric number is the value. ‘Excellent’ will get the highest score and ‘Fair’ will be our lowest score. Then we will map each label from the condition column to the numeric value and create a new column called condition_rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38334c6d-b5ab-49ab-b180-743a879db58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of label:values in order\n",
    "rating_dict = {'Excellent':5, 'New':4, 'Like New':3, 'Good':2, 'Fair':1}\n",
    "\n",
    "#create a new column \n",
    "cars['condition_rating'] = cars['condition'].map(rating_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab28776-ec4b-46c6-8a42-a5ff9ab3ae7f",
   "metadata": {},
   "source": [
    "The second approach we will show is how to utilize the sklearn.preprocessing library OrdinalEncoder. We follow a similar approach: we set our categories as a list, and then we will .fit_transform the values in our feature condition. We need to make sure we adhere to the shape requirements of a 2-D array, so you’ll notice the method .reshape(-1,1).\n",
    "\n",
    "We’ll also note, this method will not work if your feature has NaN values. Those need to be addressed prior to running .fit_transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa4c270-ed7f-4d23-a375-12c97cdb5945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using scikit-learn\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# create encoder and set category order\n",
    "encoder = OrdinalEncoder(categories=[['Excellent', 'New', 'Like New', 'Good', 'Fair']])\n",
    "\n",
    "# reshape our feature\n",
    "condition_reshaped = cars['condition'].values.reshape(-1,1)\n",
    "\n",
    "# create new variable with assigned numbers\n",
    "cars['condition_rating'] = encoder.fit_transform(condition_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc8a3dd-39ed-430d-8751-aacc4bf023bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Label encoding\n",
    "\n",
    "Now, we can talk about nominal data, and we have to approach this type of data differently than what we did with ordinal data. Our color feature has a lot of different labels, but here are the top five colors that appear in our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa893b9-07ee-4c64-837c-f7a301bbf58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cars['color'].nunique())\n",
    "# #OUTPUT \n",
    "# 19\n",
    "\n",
    "print(cars['color'].value_counts()[:5])\n",
    "# #OUTPUT\n",
    "# black     2015\n",
    "# white     1931\n",
    "# gray      1506\n",
    "# silver    1503\n",
    "# blue       869"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d3767e-1719-4d95-ab40-4f564e8cbfab",
   "metadata": {},
   "source": [
    "To prepare this feature, we still need to convert our text to numbers, so let’s do just that. We will demonstrate two different approaches, with the first one showing how to convert the feature from an object type to a categories type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2df7c-6fa8-456f-9408-f276679d32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert feature to category type\n",
    "cars['color'] = cars['color'].astype('category')\n",
    "\n",
    "# save new version of category codes\n",
    "cars['color'] = cars['color'].cat.codes\n",
    "\n",
    "# print to see transformation\n",
    "print(cars['color'].value_counts()[:5])\n",
    "# #OUTPUT\n",
    "# 2     2015\n",
    "# 18    1931\n",
    "# 8     1506\n",
    "# 15    1503\n",
    "# 3      869\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a28d90-4a4a-4924-b81a-87aa756c7ace",
   "metadata": {},
   "source": [
    "Comparing our newly transformed data to the original top 5 list, we can see Black was transformed to number 2, White was transformed to 18, and so on.\n",
    "\n",
    "However, we have created a problem for ourselves and potentially our model. We can see that ‘Blue’ cars now have a value of 3, and our model will assume that ‘Blue’ has lower precedence over the ‘Black’ car, whose color has a value of 2. Since ‘Blue’ cars = 3 and ‘White’ cars = 18, our model could actually give ‘White’ cars 6 times more weight than a ‘Blue’ car simply because of the way we encoded this feature. To combat this ordinal assumption our model will make, we should one-hot encode our nominal data, which we will cover in the next section.\n",
    "\n",
    "One more way we can transform this feature is by using sklearn.preprocessing and the LabelEncoder library. This method will not work if your feature has NaN values. Those need to be addressed prior to running .fit_transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e9f08-066a-42cc-bdcb-22322fdabe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# create encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# create new variable with assigned numbers\n",
    "cars['color'] = encoder.fit_transform(cars['color'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87de8c0-bd81-4455-a308-5f204c125e43",
   "metadata": {},
   "source": [
    "## One-hot Encoding\n",
    "\n",
    "One-hot encoding is when we create a dummy variable for each value of our categorical feature, and a dummy variable is defined as a numeric variable with two values: 1 and 0. We will continue to talk about our color feature from our used car dataset.\n",
    "\n",
    "Looking at this visual below, we can see we have ten cars in four different colors. In place of the single color column, we create four dummy variables - one new column for each color. Then the values that go into that column are binary, indicating if the car in that row is the color of the column name (1) or not (0).\n",
    "\n",
    "This approach is great for our color feature and will allow the model to see each category as its own feature and not try to create order between a “Black car” and a “Red car”. Here is how we can implement this in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8dd33-511a-4ccf-a06a-a94ee0271824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# use pandas .get_dummies method to create one new column for each color\n",
    "ohe = pd.get_dummies(cars['color'])\n",
    "\n",
    "# join the new columns back onto our cars dataframe\n",
    "cars = cars.join(ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e757a9-9369-438c-b325-8c0a09e0a041",
   "metadata": {},
   "source": [
    "## Binary encoding\n",
    "\n",
    "If we find the need to one-hot encode a lot of categorical features which would, in turn, create a sparse matrix and may cause problems for our model, a strong alternative to this issue is performing a binary encoder. A binary encoder will find the number of unique categories and then convert each category to its binary representation. \n",
    "\n",
    "To make this happen with Python we’ll use a library called category_encoders and import BinaryEncoder. We will determine which column to transform and set drop_invariant to True so it will keep the five binary columns. If it is set to the default 0, then we would have an additional column full of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8205ad-2a55-4f2e-afa9-a685f8ce64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "#this will create a new data frame with the color column removed and replaced with our 5 new binary feature columns\n",
    "colors = BinaryEncoder(cols = ['color'], drop_invariant = True).fit_transform(cars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb414b-f7f0-4865-8959-979cbb8a9e99",
   "metadata": {},
   "source": [
    "## Hashing\n",
    "\n",
    "Another option we have available to us is an encoding technique called hashing. This process is similar to one-hot encoding where it will create new binary columns, but within the parameters, you can decide how many features to output. A huge advantage is reduced dimensionality, but a large disadvantage is that some categories will be mapped to the same values. That is called collision.\n",
    "\n",
    "For example, we have 19 different colored cars. If I were to use the hash encoder and set the number of features to be 5, I will definitely have a few colors with the same hash values.\n",
    "\n",
    "Here is how we can make this work with Python. Our final result of hash_results will produce a data frame of just 5 columns, so we will want to concatenate this new data onto our original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e007f71-726d-4fac-b968-c2fefdb4a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import HashingEncoder\n",
    "\n",
    "# instantiate our encoder\n",
    "encoder = HashingEncoder(cols='color', n_components=5)\n",
    "\n",
    "# do a fit transform on our color column and set to a new variable\n",
    "hash_results = encoder.fit_transform(cars['color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46de9b3-ddcf-4a36-a00a-4acc28661901",
   "metadata": {},
   "source": [
    "Now you may be thinking, when would I use this if I’m going to lose information and my model will see brown and charcoal (or some other color combo with the same hash value) as the same thing? Well, this could be a solution to your project and dataset if you are not as interested in assessing the impact of any particular categorical value.\n",
    "\n",
    "For this example, maybe you aren’t interested in knowing which color car had an impact on your final prediction, but you want to be able to get the best performance from your model. This encoding solution may be a good approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e57a42-844e-436c-b084-04ed6e216de2",
   "metadata": {},
   "source": [
    "## Target encoding\n",
    "\n",
    "Target encoding is a Bayesian encoder used to transform categorical features into hashed numerical values and is sometimes called the mean encoder. This encoder can be utilized for data sets that are being prepared for regression-based supervised learning, as it needs to take into consideration the mean of the target variable and its correlation between each individual category of our feature. In fact, the numerical values of each category is replaced with a blend of the posterior probability of the target given a particular categorical value and the prior probability of the target over all the training data.\n",
    "\n",
    "Woah, now that was a lot of Bayesian buzzwords. How would it work with our specific color feature? It replaces each color with a blend of the mean price of that car color and the mean price of all the cars. Had it been predicting something categorical, it would’ve used a Bayesian target statistic.\n",
    "\n",
    "Some drawbacks to this approach are overfitting and unevenly distributed values that could lead to extremes. Let’s review how to implement this in Python and check out what type of numerical values it will return. Again, we’ll continue with our color feature - hope you are not yet tired of it!\n",
    "\n",
    "Say we are preparing our dataset for a regression-based supervised learning algorithm that is trying to predict the selling price.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f7687e-d43c-45a2-9748-5a77493a8101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# instantiate our encoder\n",
    "encoder = TargetEncoder(cols = 'color')\n",
    "\n",
    "# set the results of our fit_transform to a variable \n",
    "# the output will be its own pandas series\n",
    "encoder_results = encoder.fit_transform(cars['color'], cars['sellingprice'])\n",
    "\n",
    "print(encoder_results.head())\n",
    "#   color\n",
    "# 0 11761.881473\n",
    "# 1 18007.276995\n",
    "# 2 8458.251232\n",
    "# 3 14769.292595\n",
    "# 4 12691.099747"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67228522-e301-4a6e-bd6a-a4ae90639318",
   "metadata": {},
   "source": [
    "We can examine all the different values our encoder_results holds, and if we look at the output from below we can see our min is about 3,054 and our max is about 18,048. That is quite a big difference!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682a3bed-1733-461c-a60b-8b1ae8981fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all 19 unique values\n",
    "print(np.sort(encoder_results['color'].unique()))\n",
    "# OUTPUT \n",
    "# [ 3054.12209927  8088.87434555  8458.25123153  9276.78571429\n",
    "#   9867.50002121  9885.8093167  11043.90243902 11247.82608763\n",
    "#  11761.88147296 11805.06187625 12124.83443709 12376.19047882\n",
    "#  12691.09974747 13912.83399734 14769.29259451 15496.72704715\n",
    "#  17174.36440678 17176.25931731 18007.27699531 18048.52540833]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a05a7-27c2-4084-be32-143626cdda33",
   "metadata": {},
   "source": [
    "## Encoding date-time variables\n",
    "\n",
    "Every data analyst or scientist will have to work with date-time objects at some point in their career. There is so much information to be gained from date-time objects. We will work with the saledate feature - yay, a new column to explore! The very first thing we need to do is convert this to a date-time object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d33db6-4d25-4550-bdc6-6c3c614ff226",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cars['saledate'].dtypes)\n",
    "# # OUTPUT\n",
    "# dtype('O')\n",
    "\n",
    "cars['saledate'] = pd.to_datetime(cars['saledate'])\n",
    "# #OUTPUT\n",
    "# datetime64[ns, tzlocal()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6878089-6213-4139-a8ca-e815766d7824",
   "metadata": {},
   "source": [
    "Now that we have our feature set up as a datetime object, let’s demonstrate some methods we can utilize to get additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329073b-420b-45e5-8e67-25370a676a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new variable for month\n",
    "cars['month'] = cars['saledate'].dt.month\n",
    "\n",
    "# create new variable for day of the week\n",
    "cars['dayofweek'] = cars['saledate'].dt.day\n",
    "\n",
    "# create new variable for difference between cars model year and year sold\n",
    "cars['yearbuild_sold'] = cars['saledate'].dt.year - cars['year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f6ff60-0dca-44e6-85ca-3722e3530417",
   "metadata": {},
   "outputs": [],
   "source": [
    "Syntax\tDescription & Output\n",
    "df[‘col’].dt.year\tOutputs the year\n",
    "df[‘col’].dt.day\tOutputs the day number\n",
    "df[‘col’].dt.hour\tOutputs the hour from the time\n",
    "df[‘col’].dt.minute\tOutputs the minute from the time\n",
    "df[‘col’].dt.second\tOutputs the seconds from the time\n",
    "df[‘col’].dt.week\tOutputs the week ordinal of the year\n",
    "df[‘col’].dt.dayofweek\tOutputs the day of the week with Monday = 0 & Sunday = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5cae1d-76ae-4d02-a3ef-401bf690fd83",
   "metadata": {},
   "source": [
    "Additional methods can be found here.\n",
    "https://pandas.pydata.org/pandas-docs/version/0.23/api.html#datetimelike-properties"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
