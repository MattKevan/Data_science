{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6d5301-0a39-4bfd-aa76-a26eb7bfdb73",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "## Centering data\n",
    "\n",
    "Data centering involves subtracting the mean of a data set from each data point so that the new mean is 0. This process helps us understand how far above or below each of our data points is from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d4afe-df6e-4acd-b744-e419759f61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean of a dataset\n",
    "\n",
    "original_data = data.column\n",
    "mean_data = np.mean(mean_data)\n",
    "\n",
    "centered_data = original_data - mean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482ab99-8bae-4f9b-94d3-e1cf1ca29243",
   "metadata": {},
   "source": [
    "## Standardising data\n",
    "\n",
    "Standardisation (Z-Score normalisation) makes all features to be the same scale. This step is critical because some machine learning models will treat all features equally regardless of their scale. You’ll definitely want to standardize your data in the following situations:\n",
    "\n",
    "* Before Principal Component Analysis\n",
    "* Before using any clustering or distance based algorithm (think KMeans or DBSCAN)\n",
    "* Before KNN\n",
    "* Before performing regularization methods like LASSO and Ridge\n",
    "\n",
    "To standardise we center our data, then divide it by the standard deviation.\n",
    "\n",
    "z = (value - mean) / stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a8f3a-d5b0-4334-a20e-5eb31bbc843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = coffee['nearest_starbucks']\n",
    "\n",
    "#find the mean of our feature\n",
    "distance_mean = np.mean(distance)\n",
    "\n",
    "#find the standard deviation of our feature\n",
    "distance_std_dev = np.std(distance)\n",
    "    \n",
    "#this will take each data point in distance subtract the mean, then divide by the standard deviation\n",
    "distance_standardized = (distance - distance_mean) / distance_std_dev\n",
    "\n",
    "# print what type distance_standardized is\n",
    "print(type(distance_standardized))\n",
    "#output = <class 'pandas.core.series.Series'>\n",
    "\n",
    "#print the mean\n",
    "print(np.mean(distance_standardized))\n",
    "#output = 7.644158530205996e-17\n",
    "\n",
    "#print the standard deviation\n",
    "print(np.std(distance_standardized))\n",
    "#output = 1.0000000000000013\n",
    "\n",
    "# Our outputs are basically mean = 0 and standard deviation = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef1e25-9273-4d53-aa9a-2e10739eb601",
   "metadata": {},
   "source": [
    "## Standardising with Sklearn\n",
    "\n",
    "We instantiate the StandardScaler by setting it to a variable called scaler which we can then use to transform our feature. The next step is to reshape our distance array. StandardScaler must take in our array as 1 column, so we’ll reshape our distance array using the .reshape(-1,1) method. This numpy method says to take our data and give it back to us as 1 column, represented in the second value. The -1 asks numpy to figure out the exact number of rows to create based on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ba727e-bbe2-4c09-8b77-10b7d75ee6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "reshaped_distance = np.array(distance).reshape(-1,1)\n",
    "\n",
    "distance_scaler = scaler.fit_transform(reshaped_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ae429-2d40-4d3a-8505-4f35d24979a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(distance_scaler))\n",
    "#output = -9.464196275493137e-17\n",
    "print(np.std(distance_scaler))\n",
    "#output = 0.9999999999999997\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54af5c-0dbf-4f1a-95ca-4990241a1781",
   "metadata": {},
   "source": [
    "## Min-max normalisation\n",
    "\n",
    "nother form of scaling your data is to use a min-max normalization process. The name says it all, we find the minimum and maximum data point in our entire data set and set each of those to 0 and 1, respectively. Then the rest of the data points will transform to a number between 0 and 1, depending on its distance between the minimum and maximum number. We find that transformed number by taking the data point subtracting it from the minimum point, then dividing by the value of our maximum minus minimum.\n",
    "\n",
    "Mathematically a min-max normalization looks like this:\n",
    "\n",
    "Xnorm = (X - Xmin) / (Xmax - Xmin)\n",
    "\n",
    "One thing to note about min-max normalization is that this transformation does not work well with data that has extreme outliers. You will want to perform a min-max normalization if the range between your min and max point is not too drastic.\n",
    "\n",
    "The reason we would want to normalize our data is very similar to why we would want to standardize our data - getting everything on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9042ecc-816a-4753-9bda-3e8c105d32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = coffee['nearest_starbucks']\n",
    "\n",
    "#find the min value in our feature\n",
    "distance_min = np.min(distance)\n",
    "\n",
    "#find the max value in our feature\n",
    "distance_max = np.max(distance)\n",
    "\n",
    "#normalize our feature by following the formula\n",
    "distance_normalized = (distance - distance_min) / (distance_max - distance_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84040bd-3690-4f48-bd09-6a0fb0d44e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With sklearn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mmscaler = MinMaxScaler()\n",
    "\n",
    "#get our distance feature\n",
    "distance = coffee['nearest_starbucks']\n",
    "\n",
    "#reshape our array to prepare it for the mmscaler\n",
    "reshaped_distance = np.array(distance).reshape(-1,1)\n",
    "\n",
    "#.fit_transform our reshaped data\n",
    "distance_norm = mmscaler.fit_transform(reshaped_distance)\n",
    "\n",
    "#see unique values\n",
    "print(set(np.unique(distance_norm)))\n",
    "#output = {0.0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44dc2f6-8949-46e8-aa1c-e31ab9eb0c22",
   "metadata": {},
   "source": [
    "## Binning Data\n",
    "\n",
    "Binning data is the process of taking numerical or categorical data and breaking it up into groups. We could decide to bin our data to help capture patterns in noisy data. There isn’t a clean and fast rule about how to bin your data, but like so many things in machine learning, you need to be aware of the trade-offs.\n",
    "\n",
    "You want to make sure that your bin ranges aren’t so small that your model is still seeing it as noisy data. Then you also want to make sure that the bin ranges are not so large that your model is unable to pick up on any pattern. It is a delicate decision to make and will depend on the data you are working with.\n",
    "\n",
    "\n",
    "For example, our data has a range of 0 km to 8km. I wonder how our data would transform if we were to bin our data in the following way:\n",
    "\n",
    "distance < 1km\n",
    "\n",
    "1.1km <= distance < 3km\n",
    "\n",
    "3.1km <= distance < 5km\n",
    "\n",
    "5.1km <= distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e300270-346f-427a-a052-0070ae842a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 1, 3, 5, 8.1]\n",
    "\n",
    "coffee['binned_distance'] = pd.cut(coffee['nearest_starbucks'], bins, right = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3407c622-36d1-497e-ba9e-81dcc9e12c8d",
   "metadata": {},
   "source": [
    "We have 8.1 and not 8 because the pandas function we will use pd.cut() has a parameter where it will include the lower bound, and excludes the upper bound. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c8476-6ad9-432e-b3ed-f2c567e2bfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coffee[['binned_distance', 'nearest_starbucks']].head(3))\n",
    "\n",
    "#output\n",
    "#  binned_distance  nearest_starbucks\n",
    "#0      [5.0, 8.1)                  8\n",
    "#1      [5.0, 8.1)                  8\n",
    "#2      [5.0, 8.1)                  8\n",
    "\n",
    "# Plot the bar graph of binned distances\n",
    "coffee['binned_distance'].value_counts().plot(kind='bar')\n",
    " \n",
    "# Label the bar graph \n",
    "plt.title('Starbucks Distance Distribution')\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Count') \n",
    " \n",
    "# Show the bar graph \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e84dae-d6b0-43f7-941a-b3e56a36e6d2",
   "metadata": {},
   "source": [
    "## Natural Log Transformation\n",
    "\n",
    "Logarithms are an essential tool in statistical analysis and machine learning preparation. This transformation works well for right-skewed data and data with large outliers. After we log transform our data, one large benefit is that it will allow the data to be closer to a “normal” distribution. It also changes the scale so our data points will drastically reduce the range of their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e69556-79dd-4f6f-8f37-1e8100c5f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#perform the log transformation\n",
    "log_car = np.log(cars['odometer'])\n",
    "\n",
    "#graph our transformation\n",
    "plt.hist(log_car, bins = 200, color = 'g')\n",
    "\n",
    "#rotate the x labels so we can read it easily\n",
    "plt.xticks(rotation = 45)\n",
    "\n",
    "#provide a title\n",
    "plt.title('Logarithm of Car Odometers')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ea8b3-b362-4b2c-b9bc-3aa41ba9f364",
   "metadata": {},
   "source": [
    "Using a log transformation in a machine learning model will require some extra interpretation. For example, if you were to log transform your data in a linear regression model, our independent variable has a multiplication relationship with our dependent variable instead of the usual additive relationship we would have if our data was not log-transformed.\n",
    "\n",
    "Keep in mind, just because your data is skewed does not mean that a log transformation is the best answer. You would not want to log transform your feature if:\n",
    "1. You have values less than 0. The natural logarithm (which is what we’ve been talking about) of a negative number is undefined.\n",
    "2. You have left-skewed data. That data may call for a square or cube transformation.\n",
    "3. You have non-parametric data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
