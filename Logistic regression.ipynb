{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8ccb6fe-75da-4455-92ae-7f749f866472",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "Logistic regression is a supervised machine learning algorithm that predicts the probability, ranging from 0 to 1, of a datapoint belonging to a specific category, or class. These probabilities can then be used to assign, or classify, observations to the more probable group.\n",
    "\n",
    "For example, we could use a logistic regression model to predict the probability that an incoming email is spam. If that probability is greater than 0.5, we could automatically send it to a spam folder. This is called binary classification because there are only two groups (eg., spam or not spam).\n",
    "\n",
    "Some other examples of problems that we could solve using logistic regression:\n",
    "\n",
    "* Disease identification — Is a tumor malignant?\n",
    "* Customer conversion — Will a customer arriving on a sign-up page enroll in a service?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43fe924-ff00-40cb-af0c-851f5b214414",
   "metadata": {},
   "source": [
    "Linear Regression Approach\n",
    "With the data from Codecademy University, we want to predict whether each student will pass their final exam. Recall that in linear regression, we fit a line of the following form to the data:\n",
    "\n",
    "<img src=\"Assets/logistic.png\">\n",
    "\n",
    "where\n",
    "\n",
    "y is the value we are trying to predict\n",
    "b_0 is the intercept of the regression line\n",
    "b_1, b_2, … b_n are the coefficients\n",
    "x_1, x_2, … x_n are the predictors (also sometimes called features)\n",
    "\n",
    "For our data, y is a binary variable, equal to either 1 (passing), or 0 (failing). We have only one predictor (x_1): num_hours_studied. Below we’ve fitted a linear regression model to our data and plotted the results. The best fit line is in red.\n",
    "\n",
    "<<img src=\"Assets/linear_regression_ccu.svg\">\n",
    "\n",
    "We see that the linear model does not fit the data well. Our goal is to predict whether a student passes or fails; however, a best fit line allows predictions between negative and positive infinity.\n",
    "\n",
    "To build a logistic regression model, we apply a logit link function to the left-hand side of our linear regression function. Remember the equation for a linear model looks like this:\n",
    "\n",
    "<img src=\"Assets/logit.png\">\n",
    "\n",
    "This means that we are fitting the curve shown below to our data — instead of a line, like in linear regression:\n",
    "\n",
    "<img src=\"Assets/sigmoid_hours_studied_basic.svg\">\n",
    "\n",
    "Notice that the red line stays between 0 and 1 on the y-axis. It now makes sense to interpret this value as a probability of group membership; whereas that would have been non-sensical for regular linear regression.\n",
    "\n",
    "Note that this is a pretty nifty trick for adapting a linear regression model to solve classification problems! There are actually many other kinds of link functions that we can use for different adaptations.\n",
    "\n",
    "Note that we’ve replaced y with the letter p because we are going to interpret it as a probability (eg., the probability of a student passing the exam). The whole left-hand side of this equation is called log-odds because it is the natural logarithm (ln) of odds (p/(1-p)). The right-hand side of this equation looks exactly like regular linear regression!\n",
    "\n",
    "In order to understand how this link function works, let’s dig into the interpretation of log-odds a little more. The odds of an event occurring is:\n",
    "\n",
    "<img src=\"Assets/odds.png\">\n",
    "\n",
    "For example, suppose that the probability a student passes an exam is 0.7. That means the probability of failing is 1 - 0.7 = 0.3. Thus, the odds of passing are 2.33\n",
    "\n",
    "This means that students are 2.33 times more likely to pass than to fail.\n",
    "\n",
    "Odds can only be a positive number. When we take the natural log of odds (the log odds), we transform the odds from a positive value to a number between negative and positive infinity — which is exactly what we need! The logit function (log odds) transforms a probability (which is a number between 0 and 1) into a continuous value that can be positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f39a2ba0-1257-488b-9b98-c26e96a5d499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667\n",
      "-0.4054651081081643\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "odds_of_rain = 0.04 / 0.06\n",
    "print(odds_of_rain)\n",
    "# Calculate log_odds_of_rain\n",
    "\n",
    "log_odds_of_rain = np.log(odds_of_rain)\n",
    "print(log_odds_of_rain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974b4b2-3f2e-4f18-a194-edfd5e044efa",
   "metadata": {},
   "source": [
    "## Sigmoid Function\n",
    "\n",
    "Suppose that we want to fit a model that predicts whether a visitor to a website will make a purchase. We’ll use the number of minutes they spent on the site as a predictor. The following code fits the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f970e34f-b751-4d15-a277-1a8b3ae6363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(min_on_site, purchase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3328d26e-f99f-4268-a439-f4819d872e75",
   "metadata": {},
   "source": [
    "Next, just like linear regression, we can use the right-hand side of our regression equation to make predictions for each of our original datapoints as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5110df15-1ab6-4db2-940c-d897a16d0fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_odds = model.intercept_ + model.coef_ * min_on_site \n",
    "print(log_odds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0e128-8182-4640-9bf7-5b8cd1f1b1dd",
   "metadata": {},
   "source": [
    "Notice that these predictions range from negative to positive infinity: these are log odds. \n",
    "\n",
    "We can turn log odds into a probability. In Python, we can do this simultaneously for all of the datapoints using NumPy (loaded as np):\n",
    "\n",
    "The calculation that we just did required us to use something called the sigmoid function, which is the inverse of the logit function. The sigmoid function produces the S-shaped curve we saw previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5517b52-c2e0-48c2-b3d3-206cefb2fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(log_odds)/(1+ np.exp(log_odds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d228695-4f31-487d-b0d0-f1cc10795efd",
   "metadata": {},
   "source": [
    "## Fitting a model in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681b348b-dbe0-4f8e-9119-71e8d24b4944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b11b1c1-f126-4826-bb6b-c48b19a51ae4",
   "metadata": {},
   "source": [
    "After creating the object, we need to fit our model on the data. We can accomplish this using the .fit() method, which takes two parameters: a matrix of features and a matrix of class labels (the outcome we are trying to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ebf6c-c353-44cd-bf2b-a48a6bb9929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53022631-a7fd-484c-adce-cf17d61174cf",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can access a few useful attributes:\n",
    "\n",
    "model.coef_ is a vector of the coefficients of each feature\n",
    "model.intercept_ is the intercept\n",
    "The coefficients can be interpreted as follows:\n",
    "\n",
    "* Large positive coefficient: a one unit increase in that feature is associated with a large increase in the log odds (and therefore probability) of a datapoint belonging to the positive class (the outcome group labeled as 1)\n",
    "* Large negative coefficient: a one unit increase in that feature is associated with a large decrease in the log odds/probability of belonging to the positive class.\n",
    "* Coefficient of 0: The feature is not associated with the outcome.\n",
    "One important note is that sklearn‘s logistic regression implementation requires the features to be standardized because regularization is implemented by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa56067-f787-45cc-904d-18e09419b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "codecademyU = pd.read_csv('codecademyU_2.csv')\n",
    "\n",
    "# Separate out X and y\n",
    "X = codecademyU[['hours_studied', 'practice_test']]\n",
    "y = codecademyU.passed_exam\n",
    "\n",
    "# Transform X\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 27)\n",
    "\n",
    "# Create and fit the logistic regression model here:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cc_lr =  LogisticRegression()\n",
    "cc_lr.fit(X_train, y_train)\n",
    "# Print the intercept and coefficients here:\n",
    "print(cc_lr.intercept_)\n",
    "print(cc_lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afae3b0-7e06-42f2-b53b-f5f8b3b6d11a",
   "metadata": {},
   "source": [
    "## Predictions in sklearn\n",
    "Using a trained model, we can predict whether new datapoints belong to the positive class (the group labeled as 1) using the .predict() method. The input is a matrix of features and the output is a vector of predicted labels, 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa2a7d-941b-4b85-9f4a-e69e7c40312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b79a6c-69ea-4409-9abc-c655f9f443ad",
   "metadata": {},
   "source": [
    "If we are more interested in the predicted probability of group membership, we can use the .predict_proba() method. The input to predict_proba() is also a matrix of features and the output is an array of probabilities, ranging from 0 to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c32be-1265-4811-b7fb-ebc06e417961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict_proba(features)[:,1])\n",
    "# Sample output: [0.32 0.75  0.55 0.20 0.44]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27934ae-6606-4d1d-ae77-047243d93c7d",
   "metadata": {},
   "source": [
    "By default, .predict_proba() returns the probability of class membership for both possible groups. In the example code above, we’ve only printed out the probability of belonging to the positive class. Notice that datapoints with predicted probabilities greater than 0.5 (the second and third datapoints in this example) were classified as 1s by the .predict() method. This is a process known as thresholding. As we can see here, sklearn sets the default classification threshold probability as 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f18371-5d18-4c67-9fb3-56608bc8f986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and the data\n",
    "import pandas as pd\n",
    "codecademyU = pd.read_csv('codecademyU_2.csv')\n",
    "\n",
    "# Separate out X and y\n",
    "X = codecademyU[['hours_studied', 'practice_test']]\n",
    "y = codecademyU.passed_exam\n",
    "\n",
    "# Transform X\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 27)\n",
    "\n",
    "# Create and fit the logistic regression model here:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cc_lr = LogisticRegression()\n",
    "cc_lr.fit(X_train,y_train)\n",
    "\n",
    "# Print out the predicted outcomes for the test data\n",
    "print(cc_lr.predict(X_test))\n",
    "# Print out the predicted probabilities for the test data\n",
    "print(cc_lr.predict_proba(X_test)[:,1])\n",
    "# Print out the true outcomes for the test data\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cadb8c-edad-485c-a923-ea1febd60946",
   "metadata": {},
   "source": [
    "## Classification Thresholding\n",
    "As we’ve seen, logistic regression is used to predict the probability of group membership. Once we have this probability, we need to make a decision about what class a datapoint belongs to. This is where the classification threshold comes in!\n",
    "\n",
    "The default threshold for sklearn is 0.5. If the predicted probability of an observation belonging to the positive class is greater than or equal to the threshold, 0.5, the datapoint is assigned to the positive class.\n",
    "\n",
    "We can choose to change the threshold of classification based on the use-case of our model. For example, if we are creating a logistic regression model that classifies whether or not an individual has cancer, we may want to be more sensitive to the positive cases. We wouldn’t want to tell someone they don’t have cancer when they actually do!\n",
    "\n",
    "<img src=\"Assets/Threshold-01.svg\">\n",
    "\n",
    "In order to ensure that most patients with cancer are identified, we can move the classification threshold down to 0.3 or 0.4, increasing the sensitivity of our model to predicting a positive cancer classification. While this might result in more overall misclassifications, we are now missing fewer of the cases we are trying to detect: actual cancer patients.\n",
    "\n",
    "<img src = \"Assets/Threshold-02.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249de4be-e640-4da5-9f27-499c03dd604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an alternative threshold here:\n",
    "alternative_threshold = 0.6\n",
    "# Import pandas and the data\n",
    "import pandas as pd\n",
    "codecademyU = pd.read_csv('codecademyU_2.csv')\n",
    "\n",
    "# Separate out X and y\n",
    "X = codecademyU[['hours_studied', 'practice_test']]\n",
    "y = codecademyU.passed_exam\n",
    "\n",
    "# Transform X\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 27)\n",
    "\n",
    "# Create and fit the logistic regression model here:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cc_lr = LogisticRegression()\n",
    "cc_lr.fit(X_train,y_train)\n",
    "\n",
    "# Print out the predicted outcomes for the test data\n",
    "print(cc_lr.predict(X_test))\n",
    "\n",
    "# Print out the predicted probabilities for the test data\n",
    "print(cc_lr.predict_proba(X_test)[:,1])\n",
    "\n",
    "# Print out the true outcomes for the test data\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768e6f10-b047-45d4-8a99-de711a191fbe",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "When we fit a machine learning model, we need some way to evaluate it. Often, we do this by splitting our data into training and test datasets. We use the training data to fit the model; then we use the test set to see how well the model performs with new data.\n",
    "\n",
    "As a first step, data scientists often look at a confusion matrix, which shows the number of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "For example, suppose that the true and predicted classes for a logistic regression model are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8aee2-7661-4fa9-a26b-f03598e7d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 0, 1, 1, 1, 0, 0, 1, 0, 1]\n",
    "y_pred = [0, 1, 1, 0, 1, 0, 1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b352ee13-ddd4-4995-944e-221206ef539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248aa75-bace-4809-963d-b1188aded19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output\n",
    "array([[3, 2],\n",
    "       [1, 4]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed8b28c-38f5-473a-b294-845ff6041f6f",
   "metadata": {},
   "source": [
    "This output tells us that there are 3 true negatives, 1 false negative, 4 true positives, and 2 false positives. Ideally, we want the numbers on the main diagonal (in this case, 3 and 4, which are the true negatives and true positives, respectively) to be as large as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80860b1-a7b4-4ac3-9f74-ac59aa5e076d",
   "metadata": {},
   "source": [
    "## Accuracy, Recall, Precision, F1 Score\n",
    "Once we have a confusion matrix, there are a few different statistics we can use to summarize the four values in the matrix. These include accuracy, precision, recall, and F1 score. We won’t go into much detail about these metrics here, but a quick summary is shown below (T = true, F = false, P = positive, N = negative). For all of these metrics, a value closer to 1 is better and closer to 0 is worse.\n",
    "\n",
    "* Accuracy = (TP + TN)/(TP + FP + TN + FN)\n",
    "* Precision = TP/(TP + FP)\n",
    "* Recall = TP/(TP + FN)\n",
    "* F1 score: weighted average of precision and recall\n",
    "\n",
    "In sklearn, we can calculate these metrics as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682dac7-aad0-49bb-9d2b-37d9bad598d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy:\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "# output: 0.7\n",
    "\n",
    "# precision:\n",
    "from sklearn.metrics import precision_score\n",
    "print(precision_score(y_true, y_pred))\n",
    "# output: 0.67\n",
    "\n",
    "# recall: \n",
    "from sklearn.metrics import recall_score\n",
    "print(recall_score(y_true, y_pred))\n",
    "# output: 0.8\n",
    "\n",
    "# F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_true, y_pred))\n",
    "# output: 0.73"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
