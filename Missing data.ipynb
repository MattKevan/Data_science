{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "098e66f9-9e0b-4e2c-9294-4e1df4c981e5",
   "metadata": {},
   "source": [
    "# Handling missing data\n",
    "\n",
    "Missing data comes in four varieties:\n",
    "\n",
    "1. Structurally Missing Data – missing for some logical reason\n",
    "2. Missing Completely at Random (MCAR) the probability of any datapoint being MCAR is the same for all data points – this type of missing data is mostly hypothetical\n",
    "3. Missing at Random (MAR) the probability of any data point being MAR is the same within groups of the observed data – this is much more realistic than MCAR\n",
    "4. Missing Not at Random (MNAR) there is some reason why the data is missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5aafb-12f0-461c-ba8a-40a522d8d3c6",
   "metadata": {},
   "source": [
    "## Structurally missing data\n",
    "\n",
    "Sometimes when we have missing data, we aren’t surprised at all. In fact, in some scenarios, we should have missing data, because it makes sense! This is what Structurally Missing Data is, and it frequently occurs when there are one or more fields that depend on another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47cfeb-d82d-41b8-8e0c-2382a8cb2379",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Missing completely at random (MCAR)\n",
    "\n",
    "Sometimes data is just missing. It can happen for any reason, but the important thing is that it could have happened to any observation. For a given variable, every observation is equally likely to be missing. Beyond that, MCAR data is truly only MCAR if every potential observation is equally likely to be missing a value for that variable.\n",
    "\n",
    "There’s no logic, no outside force, or unseen behavior. It’s just a completely random, fluke occurrence that there isn’t data. MCAR data demands statistical perfection, which is extremely rare because more often than not, there is some unseen reason why data might be missing.\n",
    "\n",
    "This kind of missing data is less impactful on our analytics than other types, as there are a variety of techniques we can use to increase the accuracy of our analysis. We could impute the data by taking the average number of steps. We could interpolate the data by generating values based on the distribution of the existing data. Or we could delete it without making our analysis invalid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cade1a-eab8-45d2-927a-916f9fdeea3a",
   "metadata": {},
   "source": [
    "### Missing at Random (MAR)\n",
    "\n",
    "Missing at Random might be the most complex of the missing data types. If the likelihood of missingness is different for different groups, but equally likely within a group, then that data is missing at random. It’s kind of a hybrid of missing for a reason and missing completely at random.\n",
    "\n",
    "For example, several scientific studies have shown that individuals do not like to report their weight, especially when they have a Body Mass Index (BMI) outside of the “normal” range (normal is in quotations because BMI is a questionable statistic and “normal” is poorly defined, but is the label BMI scales typically use). In our study, we asked participants to report their height and weight.\n",
    "\n",
    "This is an example of Missing at Random data because we can assume that some groups are not reporting their weight for a reason, but that anyone within that group (i.e., someone with a non-“normal” BMI) has an equal probability of not reporting their weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d52e9f4-c62f-42f7-95ac-eab6aa4dafc8",
   "metadata": {},
   "source": [
    "### Missing not at random (MNAR)\n",
    "\n",
    "Finally, sometimes data is missing for a good reason that applies to all of the data. This can be the most interesting type because it’s when missingness actually tells its own story. This is when the value of the missing variable is related to the reason it’s missing in the first place!\n",
    "\n",
    "Let’s walk through an example to better understand MNAR. Participants in our study have been assigned to a local clinic to get a health reading. They will get blood pressure, height, and weight measured, and the clinician will enter notes after an interview. But a portion of the weight measurements are missing. Participants weren’t responsible for self-reporting, so this is unexpected. We might assume that they didn’t want to be weighed, but if we dive deeper into the data, we might get a different picture. We try the following groupings:\n",
    "\n",
    "last-reported weight to see if data is missing from higher or lower BMI groups\n",
    "demographics such as age, race, and gender to see if there is a pattern here\n",
    "date of data collection\n",
    "Let’s say we discovered that the missing data was all collected on the same day. In our clinic, the scales are battery-operated. If the scale runs out of batteries, someone will have to buy more. The data is missing for a reason, even though that reason is completely unrelated to our study.\n",
    "\n",
    "It’s best practice to always assume that the data is MNAR and try to uncover clues as to that reason. As the analyst, you will probably never truly know why data is missing, but finding a pattern can often inform whether the MNAR data is important to your study or not. And knowing that can help you decide what to do about the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604fed94-35b3-4dfd-998f-873c3e5df892",
   "metadata": {},
   "source": [
    "### Data about Data\n",
    "\n",
    "When trying to diagnose the type of missingness, data about the data (aka meta data) can be invaluable. The date/time data was collected, how it was collected, who collected it, where it was collected, etc. can all give invaluable clues to solving the problem of missing data.\n",
    "\n",
    "In the end, a lot of data analytics type work is solving mysteries, and The Mystery of the Missing Data is one of the best sellers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511d63fa-20d8-4425-9259-c4b7492dce24",
   "metadata": {},
   "source": [
    "## Handling Missing Data with Deletion\n",
    "\n",
    "Deletion is, quite simply, when we remove some aspect of our missing data so that our resulting dataset is as complete as possible, leading to accurate analytics. Since missing data does not provide a complete picture of what happened in our observations, we can’t rely on it for analytics, hence why deleting data can be a good solution.\n",
    "\n",
    "The big risk with deletion is that we could introduce bias, or unrepresentative data, into the dataset. If we delete too much data, or the wrong kind of data, then the resulting dataset no longer describes what actually happened accurately. In general, data is safe to delete when:\n",
    "\n",
    "1. It is either MAR or MCAR missing data. We can remove data that falls into either of these categories without affecting the rest of the data, since we assume that the data is missing at random. However, if the percentage of missing data is too high, then we can’t delete the data — we would be reducing our sample size too much.\n",
    "\n",
    "Note that every dataset or analytics use case will have a different definition of how much missing data is “too much”. For example, data used to predict credit card fraud would have a smaller tolerance for missing data than health survey data.\n",
    "\n",
    "2. The missing data has a low correlation with other features in the data. If the missing data is not important for what we’re doing, then we can safely remove that data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f5eb8-b500-4728-88ae-8a51b72c0726",
   "metadata": {},
   "source": [
    "### Types of deletion\n",
    "\n",
    "Depending on the kind of analysis we are doing, we have two available kinds of deletion available to us:\n",
    "\n",
    "#### Listwise Deletion\n",
    "\n",
    "Listwise deletion, also known as complete-case analysis, is a technique in which we remove the entire observation when there is missing data. This particular technique is usually employed when the missing variable(s) will directly impact the analysis we are trying to perform, usually with respect to MAR or MCAR missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d915f96-c9dc-49a0-8936-bbe8e7d8b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True) \n",
    "# Drops entire rows with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e15141-14d4-4e1f-8a2b-4b6c9c13a013",
   "metadata": {},
   "source": [
    "In general, we should be cautious when using listwise deletion, as we lose a lot of information when we remove an entire row. When we remove rows like this, we decrease the amount of data that we can use for analysis. This means we would have less confidence in the accuracy of any conclusions we draw from the resulting dataset.\n",
    "\n",
    "As a best practice, we should only use listwise deletion when the number of rows with missing data is relatively small to avoid significant bias. Every dataset will have a different context for how much data is safe to remove. A safe place to start is assuming that if less than 5% of data is missing, then we are safe to use listwise deletion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a808c87a-a671-4119-a55d-59f3949f106b",
   "metadata": {},
   "source": [
    "#### Pairwise Deletion\n",
    "\n",
    "Pairwise deletion is an alternative to listwise deletion, and looks for context to what we are trying to analyze. In pairwise deletion, we only remove rows when there are missing values in the variables we are directly analyzing. Unlike listwise deletion, we do not care if other variables are missing, and can retain those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ef402-284b-4478-bace-be9ada812aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['Height','Education'], #only looks at these two columns\n",
    "            inplace=True, #removes the rows and keeps the data variable\n",
    "            how='any') #removes data with missing data in either field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d3282-0aee-4bb2-8dac-932c8b01f7d7",
   "metadata": {},
   "source": [
    "#### Dropping Variables\n",
    "\n",
    "There is another tactic we could employ: to drop a variable entirely. If a variable is missing enough data, then we really don’t know enough about that variable to use it in our analysis, so we can’t be confident in any conclusions we draw from it.\n",
    "\n",
    "While this may sound easier than the other solutions mentioned and possibly effective, we generally don’t want to drop entire variables. Why? In most contexts, having more data is always a good idea and we should work to retain it if possible. The more data we have, the more confidence we can have that our conclusions are actually happening, and not due to random chance. We should only drop a variable as a last resort, and if that variable is missing a very significant amount of data (at least 60%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42872b44-08a0-45ef-a773-7b69bd131beb",
   "metadata": {},
   "source": [
    "## Single Imputation\n",
    "\n",
    "When data is missing in time-series data, we have the advantage of context. That is, because the data shows observations of the same event over time, we can learn about what is happening and make an educated guess as to what might have happened in our missing data. The methods that we will talk about in this article are specific to time-series data because of this property.\n",
    "\n",
    "Is it MNAR?\n",
    "Before we can start describing techniques, we must verify that our missing data can be categorized as MNAR — these techniques assume that to be the case. It can be tricky to understand that the data is missing in a non-random manner. How exactly can we verify this? There are two key aspects to be able to accurately describe missing data as MNAR:\n",
    "\n",
    "Use domain knowledge: Probably the quickest way to identify MNAR is by having extensive knowledge of the data and industry we are working in. Missing data that falls into MNAR will look and feel different from “normal” data in our dataset.\n",
    "\n",
    "With domain knowledge, either from ourselves or someone on our team, we could identify the cause for missing data. For example, someone might know that data in a survey is missing in a particular column because the participant was either too embarrassed to answer, or didn’t know the answer. This would let us know that the data is MNAR.\n",
    "Analyze the dataset to find patterns: As we explore, filter, and separate our data, we should ultimately be able to identify something about our missing data that doesn’t line up with everything else. For example, if we have some survey data, we might find that our missing data almost exclusively comes from men older than 55 years old. If we see a pattern like this emerge, we can confidently say it is MNAR.\n",
    "\n",
    "#### LOCF\n",
    "\n",
    "LOCF stands for Last Observation Carried Forward. With this technique, we are going to fill in the missing data with the previous value. LOCF is used often when we see a relatively consistent pattern that has continued to either increase or decrease over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6c8a2-3171-4c0d-af34-43fb3960ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df['comfort'].ffill(axis=0, inplace=True)\n",
    "# Applying Forward Fill (another name for LOCF) on the comfort column\n",
    "\n",
    "# Numpy \n",
    "impyute.imputation.ts.locf(data, axis=0)\n",
    "# Applying LOCF to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaa4c9d-96a3-4b31-b9a5-61aeba2de919",
   "metadata": {},
   "source": [
    "#### NOCB\n",
    "\n",
    "NOCB stands for Next Observation Carried Backward, and it solves imputation in the opposite direction of LOCF. NOCB is usually used when we have more recent data, and we know enough about the past to fill in the blanks that way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b2560-d309-499c-947b-c6e58ade55df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas \n",
    "df['comfort'].bfill(axis=0, inplace=True)\n",
    "\n",
    "#Numpy\n",
    "impyute.imputation.ts.nocb(data, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f160e5-3c80-40c4-af98-355db84111dc",
   "metadata": {},
   "source": [
    "#### Other alternatives\n",
    "\n",
    "With LOCF and NOCB, we assume that the data either directly before or after is accurate enough to fill in any surrounding missing data. There are, however, other approaches we can try depending on our data. As with the previous methods of single imputation, deciding which method to use requires domain knowledge to correctly fill in the missing values.\n",
    "\n",
    "One such alternative is BOCF, or Baseline Observation Carried Forward. In this approach, the initial values for a given variable are applied to missing values. This is a common approach in medical studies, particularly in drug studies. For example, we could assume that missing data for a reported pain level could return to the baseline value. This would happen if a drug were not working as well, so it could be a safe assumption to make if the data is trending in that direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400b1560-463b-4033-9558-910a2e62899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the first (baseline) value for our data\n",
    "baseline = df['concentration'][0]\n",
    "\n",
    "# Replace missing values with our baseline value\n",
    "df['concentration'].fillna(value=baseline, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71932a-11a9-402d-a82c-c61592956ebf",
   "metadata": {},
   "source": [
    "Another alternative for single imputation is WOCF, or Worst Observation Carried Forward. With this kind of imputation, we want to assume that the data is the worst possible value. This would be useful if the purpose of our analysis was to record improvement in some value (for example, if we wanted to study if a treatment was helping a particular patient’s condition). By filling in data with the worst value, then we can reduce potentially biased results that didn’t actually happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582e3661-077d-4276-b4c3-e88aa11b594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate worst pain value (in this case, the highest)\n",
    "worst = df['pain'].max()\n",
    "\n",
    "# Replace all missing values with the worst value\n",
    "df['pain'].fillna(value=worst, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d56725-71e9-418e-9cf8-4d78f4d0a919",
   "metadata": {},
   "source": [
    "#### What are the disadvantages?\n",
    "Single imputation methods are very useful when we have missing data. By using these methods, we can easily and quickly fill in missing data from contextual information in the dataset. There are, however, some disadvantages to using single imputation methods.\n",
    "\n",
    "The biggest disadvantage of these methods is the potential for adding bias into a dataset. When we use single imputation, we assume that the data we are using to fill in the blanks is reliable and accurate for that observation. This isn’t always the case, however. Data often will change in unexpected ways. Single imputation will ignore these potential changes and will “smooth” out our data, which could lead to inaccurate results.\n",
    "\n",
    "In general, single imputation can be an effective technique to handle missing data for our time-series datasets. While it might not be the most sophisticated approach, it can be a useful quick-fix in the right circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f117d-ed93-4028-810f-c4c6825a7a48",
   "metadata": {},
   "source": [
    "## Multiple imputation\n",
    "\n",
    "A technique for filling in missing data, in which we replace the missing data multiple times. Multiple imputation, in particular, is used when we have missing data across multiple categorical columns in our dataset. After we have tried different values, we use an algorithm to pick the best values to replace our missing data. By doing this, we are able to, over time, find the correct value for our missing data.\n",
    "\n",
    "There are numerous algorithms that we can use, all with their own advantages and disadvantages. Overall, the general process for multiple imputation follows the below flow chart.\n",
    "\n",
    "After each iteration, our predicted values for each variable should get more and more accurate, since the models continue to refine to better fit our dataset. The goal of multiple imputation is to fill in the missing data so that it can find a model — typically either a normal or chi-square model — to best fit the dataset.\n",
    "\n",
    "### When to use it\n",
    "Multiple imputation is a powerful technique for replacing missing data, but there are some requirements and considerations to take before using multiple imputation.\n",
    "\n",
    "Multiple imputation is best for MAR data, so we should ensure that our data fits that description. With MAR missing data, there is an assumption that there is an underlying reason to have missing data, and we have a good understanding of why that data is missing. Since it is not completely random, using random data to fill in the blanks is not sufficient, so we must use the context of the rest of the data to help.\n",
    "\n",
    "Assuming we meet the criteria for using multiple imputation, our dataset will receive a couple key benefits.\n",
    "\n",
    "We can safely assume that our data won’t be biased, since we start the process off with a random assignment of values for the missing data.\n",
    "Because the goal of multiple imputation is to have a model that fits the data, we can be pretty confident that the resulting data will be a close approximation of the real data. This would include calculations like standard error and overall statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f133c159-a977-4b5f-bea1-e92df44ee2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      X     Y    Z\n",
      "0   5.4  18.0  7.6\n",
      "1  13.8  27.4  4.6\n",
      "2  14.7  17.4  4.2\n",
      "3  17.6  18.3  5.6\n",
      "4  11.2  49.6  4.7\n",
      "5   1.1  48.9  8.5\n",
      "6  12.9  17.4  3.5\n",
      "7   3.4  13.6  5.7\n",
      "8  11.2  16.1  1.8\n",
      "9  10.2  42.7  4.7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Create the dataset as a Python dictionary\n",
    "d = {\n",
    "    'X': [5.4,13.8,14.7,17.6,np.nan,1.1,12.9,3.4,np.nan,10.2],\n",
    "    'Y': [18,27.4,np.nan,18.3,49.6,48.9,np.nan,13.6,16.1,42.7],\n",
    "    'Z': [7.6,4.6,4.2,np.nan,4.7,8.5,3.5,np.nan,1.8,4.7]\n",
    "}\n",
    "\n",
    "dTest = {\n",
    "    'X': [13.1, 10.8, np.nan, 9.7, 11.2],\n",
    "    'Y': [18.3, np.nan, 14.1, 19.8, 17.5],\n",
    "    'Z': [4.2, 3.1, 5.7,np.nan, 9.6]\n",
    "}\n",
    "\n",
    "# Create the pandas DataFrame from our dictionary\n",
    "df = pd.DataFrame(data=d)\n",
    "dfTest = pd.DataFrame(data=dTest)\n",
    "\n",
    "# Create the IterativeImputer model to predict missing values\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "\n",
    "# Fit the model to the test dataset\n",
    "imp.fit(dfTest)\n",
    "\n",
    "# Transform the model on the entire dataset\n",
    "dfComplete = pd.DataFrame(np.round(imp.transform(df),1), columns=['X','Y','Z'])\n",
    "\n",
    "print(dfComplete.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b11b94a-2118-4b28-b664-daa95af8672f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0642312e-2a63-493f-a085-91ef5b2468e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
